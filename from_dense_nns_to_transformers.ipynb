{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-09T13:56:42.714665400Z",
     "start_time": "2023-10-09T13:56:27.864741700Z"
    },
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "mrWjW2dsRdICQ0xSRRon8m",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "fDLNAGVQBkXcqHfsOA6djC",
     "type": "MD"
    }
   },
   "source": [
    "# This notebook contains notes on all steps necessary to understand basic Transformer architecture.\n",
    "\n",
    "## Keras basics\n",
    "Keras gives simple API to programming neural networks with tensorflow. This section will provide quick overview of most important features used in code. More detailed explanations can be found [here](https://www.tensorflow.org/guide/keras). Important feature (in terms of understanding Keras code) is that Keras doesn't need input shape of NN to be specified, it can be determined during runtime, so usually Input layer is omitted.\n",
    "### How to create models - Sequential model\n",
    "Sequential model provides simple way to program neural networks that can be modeled as stack of layers. Example code that models NN with hidden layer of 3 neurons (relu activation) and output of 2 neurons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-09T13:18:05.960901300Z",
     "start_time": "2023-10-09T13:18:05.935122500Z"
    },
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "OoroH91kDL7ZEMwbPq0F8b",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Dense(3, activation=\"relu\", name=\"hidden\"),\n",
    "        tf.keras.layers.Dense(2, name=\"output\"),\n",
    "    ])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "m4Bf8ndpLnI8lx9yvxxKYd",
     "type": "MD"
    }
   },
   "source": [
    "Model created above can be called on input $x$ with shape 3x3 (for example batch_size x n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-09T13:18:05.943133500Z"
    },
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "HXJ78obn5Fje4Sro5vW8yn",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "x = tf.ones((3, 3))\n",
    "y = model(x)\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "K8v1MDgMPm1nqFrKlSn5Fn",
     "type": "MD"
    }
   },
   "source": [
    "\n",
    "\n",
    "Another way to create such model is to create and add layers separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-09T13:18:05.951126800Z"
    },
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "WV84xFWrHJsYDO2WjV7iUh",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "hidden = tf.keras.layers.Dense(3, activation=\"relu\", name=\"hidden\")\n",
    "model.add(hidden)\n",
    "out = tf.keras.layers.Dense(2, name=\"output\")\n",
    "model.add(out)\n",
    "\n",
    "\n",
    "x = tf.ones((3, 3))\n",
    "y = model(x)\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "2JkDGfINTUY3dpcKxMqIjM",
     "type": "MD"
    }
   },
   "source": [
    "### How to create models - Functional API\n",
    "Functional API enables creation of more complicated models - ones that cannot be modeled as stack of layers (for example ones that take more than one input). Let's create model that takes 10-dimensional vector as input, feeds it into dense layer with 4 neurons and outputs 1 neuron. Here we create layers separately and link them using call operator. Since here input is defined explicitly, we can show summary before feeding real data to model - that's why layer output shapes have None - to accommodate batch size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-09T13:18:05.952899800Z"
    },
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "QR0G0auv9vpdEOop8s64J0",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(10,), name=\"inputs\")\n",
    "hidden = tf.keras.layers.Dense(4, activation=\"relu\", name=\"hidden\")(inputs)\n",
    "out = tf.keras.layers.Dense(1, activation=\"linear\", name=\"out\")(hidden)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=out)\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "lcB6GGIEFZyFO3DrQwDknu",
     "type": "MD"
    }
   },
   "source": [
    "### How to train models\n",
    "To train model, first it needs to be compiled. We can specify optimizer used and cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-09T13:18:05.960901300Z"
    },
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "rJPOtSkYs5gR9GMfE10OSJ",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.MeanSquaredError())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "flZXsewhXLoc5xTcwtcp2X",
     "type": "MD"
    }
   },
   "source": [
    "Then we can fit model. Train data, validation data, batch size and number of epochs can be specified. Train data can be either passed as x_train and y_train or as tf.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-09T13:18:05.969531500Z"
    },
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "1WrB5swJmpawlspg9mDE3T",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "# prepare data\n",
    "x_train = tf.ones((10, 10))\n",
    "y_train = tf.ones((10, 1))\n",
    "x_val = tf.ones((1, 10))\n",
    "y_val = tf.ones((1, 1))\n",
    "\n",
    "print(\"fit by passing x_train, y_train\")\n",
    "model.fit(x_train, y_train, batch_size=2,  epochs=1, validation_data=(x_val, y_val))\n",
    "\n",
    "print(\"fit with dataset\")\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(2)\n",
    "model.fit(dataset, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "jpXn2olRdKmMwiDWrJidTs",
     "type": "MD"
    }
   },
   "source": [
    "## Dense NNs vs RNNs\n",
    "Note: biases are omitted in all calculations and pictures to simplify a little bit\n",
    "\n",
    "### Simplest case - Dense Neural Network\n",
    "The simplest NN is a dense one - it takes some inputs $x$, has some hidden neurons $h$ and produces some output $y$. It has 2 sets of weights:\n",
    "- $W_{xh}$ that are applied to input\n",
    "- $W_{hy}$ that are applied to hidden state before producing output\n",
    "  \n",
    "Picture below portraits dense neural network with one hidden layer\n",
    "\n",
    "![Simple NN](./images/obraz_2023-07-31_194835707.png)\n",
    "\n",
    "Or, in simplified form it can be pictured as:\n",
    "\n",
    "![Simplified NN picture](./images/obraz_2023-07-31_195230251.png)\n",
    "\n",
    "During forward pass such neural network does following computations:\n",
    "1) compute hidden activations $h = tanh(x \\cdot W_{xh})$*\n",
    "2) produce output $ y = h \\cdot W_{hy}$\n",
    "   \n",
    "\\* tanh is used as activation function, but it can be anything (relu/sigmoid)\n",
    "\n",
    "In dense NNs hidden layer can have different shape than input. If for example we have 10-dimensional input, 4 hidden neurons and 1 output, shapes of NN components are:\n",
    "- $x$ = (n_batches, 10) - input\n",
    "- $W_{xh}$ = (10, 4)\n",
    "- $h$ = (n_batches, 4) - hidden layer output\n",
    "- $W_{hy}$ = (4, 1)\n",
    "- $y$ = (n_batches, 1) - output\n",
    "\n",
    "Below is example code that defines such network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-09T13:18:06.131632300Z",
     "start_time": "2023-10-09T13:18:05.978477Z"
    },
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "GY5VzT1gtgZH6jYqUU8zic",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(10,), name=\"inputs\")\n",
    "hidden = tf.keras.layers.Dense(4, activation=\"relu\", name=\"hidden\")(inputs)\n",
    "out = tf.keras.layers.Dense(1, activation=\"linear\", name=\"out\")(hidden)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=out)\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "0ktoDvZWPYBNSCEATGnCOo",
     "type": "MD"
    }
   },
   "source": [
    "To apply dense NNs for time series prediction, first data needs to be properly prepared due to the fact, that dense NNs take input of fixed size. If for example we have 14 data points and we want to predict 1 timestamp ahead based on 10 previous timestamps, 3 training samples would be generated from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-09T13:18:05.994475900Z"
    },
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "JAEMlYzzYF2ZCftWo9SFKX",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "window_size = 10\n",
    "\n",
    "raw_data = np.random.rand(14)\n",
    "dataset = np.lib.stride_tricks.sliding_window_view(raw_data[:-1], (window_size + 1, ))\n",
    "x = dataset[:, :-1]\n",
    "y = dataset[:, -1]\n",
    "print(f'X shape: {x.shape}, Y shape: {y.shape}. Got {y.shape[0]} batches')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "RM4m01pfZqaWoNdSEMaAQr",
     "type": "MD"
    }
   },
   "source": [
    "### Recurrent Neural Networks\n",
    "Most basic RNN can be defined as series of cells, where each cell takes input $x$, returns output $y$ and in each time step passes hidden state $h$ to next time step. This neural network architecture includes 3 sets of weights: \n",
    "- $W_{hh}$ that are applied to hidden state passed from previous timestep\n",
    "- $W_{xh}$ that are applied to input at each timestep\n",
    "- $W_{hy}$ that are applied to hidden state before producing output at each timestep\n",
    "\n",
    "\n",
    "Below picture shows whole architecture in a compact way.\n",
    "\n",
    "![Compact picture of RNN](./images/obraz_2023-07-31_195840738.png)\n",
    "\n",
    "One of the most important concepts in RNNs is unrolling - that means picturing network cells for different timestamps as separate neurons. Below picture shows network presented above unrolled for 3 timesteps.\n",
    "\n",
    "![Unrolled picture of RNN](./images/obraz_2023-07-31_200503554.png)\n",
    "\n",
    "\n",
    "Network takes as input initial hidden state -  $h_{start}$, usually initialized to 0.\n",
    "In timestep $t = 1$ network takes two inputs - $h_0$ (hidden state from previous timestep) and $x_1$ (input at timestep 1), produces two outputs - $h_1$ that is passed to next timestep and $y_1$ that is output for this timestep.\n",
    "\n",
    "During forward pass at timestep $t$ such neural network does following computations:\n",
    "1) calculate hidden activations $h_t = tanh(W_{hh} \\cdot h_{t - 1} + W_{xh} \\cdot x_t )$*\n",
    "2) produce output $ y_t = h_t \\cdot W_{hy}$\n",
    "\n",
    "\\* tanh is used as activation function, but it can be anything (relu/sigmoid)\n",
    "\n",
    "These steps are calculated at each timestep using the same wieghts. \n",
    "\n",
    "Like in dense NNs, hidden layer can have different shape than input. If for example we have 1-D input, 4 hidden units, which implies 4-D output. If we want to get 1-D output dense layer on top of RNN is required. Shapes of whole network (RNN + Dense) components are:\n",
    "\n",
    "Shapes below are result of setting return_sequences for RNN to False - that means that RNN layer will return only last output\n",
    "\n",
    "- $x$ = (n_batches, n_timesteps, 1) - input\n",
    "- $W_{xh}$ = (1, 4)\n",
    "- $h$ = (1, 4) - hidden layer output - calculated at each step\n",
    "- $W_{hy}$ = (4, 4)\n",
    "- $W_{hy}$ = (4, 4)\n",
    "- $y_{hidden}$ = (n_batches, 4) - output - result of running n_batches through RNN\n",
    "- $W_{dense}$ = (4,  1) - weights of dense layer\n",
    "- $y_{dense}$ = (n_batches, 1) - output\n",
    "\n",
    "If we set return_sequences to True shapes are as follows:\n",
    "\n",
    "- $x$ = (n_batches, n_timesteps, 1) - input\n",
    "- $W_{xh}$ = (1, 4)\n",
    "- $h$ = (1, 4) - hidden layer output - calculated at each step\n",
    "- $W_{hy}$ = (4, 4)\n",
    "- $W_{hy}$ = (4, 4)\n",
    "- $y_{hidden}$ = (n_batches, n_timesteps, 4) - output - result of running n_batches through RNN\n",
    "- $W_{dense}$ = (4,  1) - weights of dense layer, applied at each timestep\n",
    "- $y_{dense}$ = (n_batches, n_timesteps, 1) - output\n",
    "\n",
    "\n",
    "Below is example code that defines such networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "Rx6dchiMkrrtnDaNSP88qF",
     "type": "MD"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-09T13:18:06.023791100Z"
    },
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "R3SDzTZMUMh0TE9A3RaNEg",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "timesteps = 5\n",
    "# not returning sequences\n",
    "inputs = tf.keras.Input(shape=(timesteps, 1), name=\"inputs\")\n",
    "hidden = tf.keras.layers.SimpleRNN(4, name=\"hidden\", return_sequences=False)(inputs)\n",
    "out = tf.keras.layers.Dense(1, activation=\"linear\", name=\"out\")(hidden)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=out)\n",
    "model.summary()\n",
    "# returning sequences\n",
    "inputs = tf.keras.Input(shape=(timesteps, 1), name=\"inputs\")\n",
    "hidden = tf.keras.layers.SimpleRNN(4, name=\"hidden\", return_sequences=True)(inputs)\n",
    "out = tf.keras.layers.Dense(1, activation=\"linear\", name=\"out\")(hidden)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=out)\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "QKjcaAjqxCYoi4NqflmXPR",
     "type": "MD"
    }
   },
   "source": [
    "#### Stacking RNNs\n",
    "RNNs can be stacked to create multiple layers. In such configuration at each timestep output from first RNN is passed as input for another one. In such case all layers that pass outputs to another layer, must return sequences, so each timestep is passed.\n",
    "If we would want to pass just last output, it would create encoder - decoder architecture, which will be explained later. Below is example architecture definition that stacks RNN layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-09T13:18:06.070524600Z"
    },
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "fygWKNInYXElTGxkJnIMTX",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "timesteps = 5\n",
    "inputs = tf.keras.Input(shape=(timesteps, 1), name=\"inputs\")\n",
    "hidden1 = tf.keras.layers.SimpleRNN(4, name=\"hidden1\", return_sequences=True)(inputs)\n",
    "hidden2 = tf.keras.layers.SimpleRNN(5, name=\"hidden2\", return_sequences=True)(hidden1)\n",
    "hidden3 = tf.keras.layers.SimpleRNN(6, name=\"hidden3\", return_sequences=False)(hidden2)\n",
    "out = tf.keras.layers.Dense(1, activation=\"linear\", name=\"out\")(hidden3)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=out)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "fRTbyEUuZKeHq4ILTjcA3p",
     "type": "MD"
    }
   },
   "source": [
    "## LSTMs\n",
    "### General concept\n",
    "Similarly to how simple RNNs, LSTMs are built from cells. Here cell structure is a little more complicated â€” each cell has 3 inputs and 2 outputs. Outputs of each cell are:\n",
    "- $c$ - called \"state\", because it transports data between timesteps, while undergoing little modification at each timestep, which allows for capturing long time dependencies (also called \"Long Term Memory\")\n",
    "- $h$ - being output of given cell. It is modified at every step depending on $c$, previous timestep's output and current input, which allows for capturing short time dependencies (also called \"Short Term Memory\")\n",
    "\n",
    "Inputs for each cell are $h$ and $c$ from previous cell and $x$ at given timestep.\n",
    "\n",
    "Having this 2 tensors $h$, $c$ passed between timesteps allows this architecture to learn both long and short term dependecies in data - that's why it's called Long Short Term Memory.\n",
    "\n",
    "### Cell's internal structure\n",
    "Each cell has 3 main components:\n",
    "- forget gate responsible for selecting which elements of state passed from previous timestep will be forgotten and which will be used to produce output and passed to next timestep\n",
    "- input gate responsible for updating state\n",
    "- output gate responsible for deciding how state will affect output and producing it\n",
    "\n",
    "Each component is modeled using neural network and has it's own set of weights.\n",
    "\n",
    "- Forget gate is single neural network that on given timestep $t$, based on $h_{t-1}$, $x_t$ and weights $w_f$ produces vector $f$. Sigmoid is used as activation function, so $f$ elements $\\in <0, 1>$. $f$ is multiplied element-wise with state $c_{t-1}$ - this operation decides which elements of $c_{t-1}$ will be forgotten.\n",
    "- Input gate consists of 2 neural networks, both taking $h_{t-1}$, $x_t$ as input.\n",
    "    - First one produces candidate state $C$ based on $W_c$ set of weights. It uses tanh activation function (since $c$ elements $\\in <-1; 1>$)\n",
    "    - Second one is responsible for deciding how candidate state $C$ will affect state $c$ passed to next step and used to produce output. Based on weights $W_i$ it produces tensor $i$, which elements $\\in <0; 1>$ because sigmoid activation is used.\n",
    "    - Outputs of these networks are multiplied with each other element-wise and then added to $c_{t-1} \\cdot f$ (produced by forget gate) in order to produce state $c_t$. \n",
    "- Output gate consists of single neural network, with sigmoid activation, which decides how state $c_t$ will affect output $h_t$. Based on weights $w_o$ it produces $o$ vector, which is multilpied element-wise with $tanh(c_t)$ in order to produce output $h_t$. $tanh$ is used here to squeeze $c$ to $<-1; 1>$.\n",
    "\n",
    "Picture below ilustrates in greater detail how LSTM cell works.\n",
    "![LSTM cell](./images/obraz_2023-08-02_200510371.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "1G1bJLJrbNmNq3dh89lqIV",
     "type": "MD"
    }
   },
   "source": [
    "### What computations are done in every cell\n",
    "1) $f_t = sigmoid(w_f \\cdot [h_{t-1}, x_t])$\n",
    "2) $c_t = c_{t-1} \\cdot f_t$\n",
    "3) $C_t = tanh(w_c \\cdot [h_{t-1}, x_t])$\n",
    "4) $i_t = sigmoid(w_i \\cdot [h_{t-1}, x_t])$\n",
    "5) $c_t$ += $i_t \\cdot C_t$\n",
    "6) $o_t = sigmoid(w_o \\cdot [h_{t-1}, x_t])$\n",
    "7) $h_t = o_t \\cdot tanh(c_t)$\n",
    "\n",
    "### Example\n",
    "In keras LSTMs are used exactly the same as regular RNNs, so parameter passed as number of units determines dimensionality of $h_t$.\n",
    "\n",
    "Let's say we want to create a network that based on 1-D data predicts next step in that data. Our network will have 2 stacked LSTMs - one with 10 hidden units, second with 15 units and dense layer to aggregate this data into 1 predicted point in time. We can set return_sequences=True for last LSTM layer, so our model will learn to predict next value at each timestep, which means we can predict sequences of arbitrary length by just feeding previous output as input to next step. Then if our input series is $[1, 2, 3, 4, 5, 6]$ we can set model inputs and outputs to $x = [1, 2, 3, 4, 5]$, $y = [2, 3, 4, 5, 6]$. If we didn't set return_sequences to True we would teach our model to first take $n$ timesteps, then predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-09T13:18:06.074986600Z"
    },
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "Z1rFFHctMRDh7yxtQWzY63",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "timesteps = 5\n",
    "inputs = tf.keras.Input(shape=(timesteps, 1), name=\"inputs\")\n",
    "hidden1 = tf.keras.layers.LSTM(10, name=\"hidden1\", return_sequences=True)(inputs)\n",
    "hidden2 = tf.keras.layers.LSTM(15, name=\"hidden2\", return_sequences=True)(hidden1)\n",
    "out = tf.keras.layers.Dense(1, activation=\"linear\", name=\"out\")(hidden2)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=out)\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "q0bVbaZF1bke9ma8BVySST",
     "type": "MD"
    }
   },
   "source": [
    "Let's take a closer look at weights of first LSTM.\n",
    "Inputs for it will be shape $(n, 5, 1)$, and cell has 10 units so\n",
    "LSTM cell at given timestep will take following parameters:\n",
    "\n",
    "(omitting batch size)\n",
    "- $h_t = (10)$\n",
    "- $c_t = (10)$\n",
    "- $x_t = (1)$\n",
    "- $w_f = w_i = w_c = w_o =(11, 10) $ - shape of $h_t$ joined with $x_t$ by shape of $c_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "wHG6uWy8sqhlJf85DqMiNU",
     "type": "MD"
    }
   },
   "source": [
    "## Autoencoders\n",
    "### Architecture\n",
    "After understanding LSTMs, autoencoders are quite simple architecture. Problem that they solve is creating output of different length than input. With RNNs we can predict multiple timesteps ahead, but it's done by running multiple timesteps with predictions as inputs to next timesteps, but we can't explicitly teach the model to predict $n$ timesteps ahead. The main idea behind autoencoders it to create 2 separate networks : \n",
    "- encoder, that encodes the data into context vector,that contains all the information necessary to make predictions\n",
    "- decoder, that based on context vector produces final predictions\n",
    "\n",
    "When it comes to time series processing, basic encoder-decoder setups use RNNs (or their variations like LSTMs) as building blocks for both parts of the architecture. Context vector is usually final hidden state of encoder, with which decoder hidden state is initialized.\n",
    "\n",
    "Encoder-decoder architecture is derived from seq2seq problems like translation, where during training decoder input at timestamp $t$ is set to correct translated value of previous timestep $y_{t-1}$ - setting inputs to decoder in such way is called teacher forcing. In time series prediction tasks, teacher forcing isn't ideal way to teach decoder, since usually time series value at $t$ is highly correlated with value at $t-1$, so decoder learns to make predictions based on previous value (which during inference will be predicted value of previous timestep, not true one) instead of context vector. This leaves us with 2 possible ways of setting decoder inputs during training - either to 0 or to elements of context vector.\n",
    "\n",
    "Picture below portraits high-level architecture of encoder-decoder setup\n",
    "\n",
    "![encoder decoder architecture](./images/obraz_2023-08-02_213214447.png)\n",
    "\n",
    "### Example\n",
    "In this example we will define encoder-decoder architecture with 10-unit LSTMs as both encoder and decoder. Such setup can predict arbitrary number of timesteps ahead based on arbitraty number of input timesteps. Nubmer of input and output timesteps will be defined by dataset on which it will be trained. Here input timesteps number will be set to 5 in order to better ilustrate layer output dimensions and output timesteps will be set to 2, to simplify code (for arbitrary number of outputs custom layer would need to be defined, here we'll just use RepeatVector to serve as decoder inputs at each timestep)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-09T13:18:06.084200Z"
    },
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "Sf2iu73jF4Yeoh4DQ3WkNY",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "input_timesteps = 5\n",
    "output_timesteps = 2\n",
    "batch_size = 1\n",
    "\n",
    "encoder_inputs =  tf.keras.layers.Input(shape=(input_timesteps, 1))\n",
    "encoder = tf.keras.layers.LSTM(10, return_state=True, return_sequences=False)\n",
    "# state_h and state_c are context vector\n",
    "# encoder_outputs are discarded (they are the same as state_h since LSTM is used)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "# using hidden state h from encoder as inputs at each decoder timestep\n",
    "# decoder =  tf.keras.layers.RepeatVector(output_timesteps)(state_h)\n",
    "# using zeros as inputs at each decoder timestep\n",
    "decoder =  tf.keras.layers.RepeatVector(output_timesteps)(tf.zeros((batch_size, 1)))\n",
    "\n",
    "decoder_lstm =  tf.keras.layers.LSTM(10, return_sequences=True, return_state=False)\n",
    "decoder = decoder_lstm(decoder, initial_state=[state_h, state_c])\n",
    "\n",
    "# using dense layer to aggregate 10-D LSTM output\n",
    "# using TimeDistributed to make it clear that dense layer is applied to every timestep separately\n",
    "# but it does not make any real difference (i believe)\n",
    "out =  tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1))(decoder)\n",
    "model =  tf.keras.models.Model(encoder_inputs, out)\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "cibVb4UAZ1UstMhs99yTsV",
     "type": "MD"
    }
   },
   "source": [
    "## Attention\n",
    "### General idea\n",
    "Attention is somewhat complicated way to express a simple idea - having vector $x$ and $y$ we would want to calculate how diffenent elements of $x$ influence different elements of $y$ (so in other words we want to assign different weight to each element of $x$ depending on how much it influences each element of $y$). Let's put our weights in vector $a$ of size $size(x)$ x $size(y)$, so for each element of $x$ each element of $y$ has some weight Now let's write it down mathematically.\n",
    "\n",
    "$a_{i, j} = attention(x_i, y_j)$, where $attention$ is function telling us how similar it's arguments are.\n",
    "\n",
    "The simplest way of telling how similar two numbers are is multiplying them, so in the simplest case $a$ could be calculated as a dot product of $x$ and $y$.\n",
    "\n",
    "$a = x \\cdot y $\n",
    "\n",
    "*Note: attention function can be also more complicated - for example modeled by neural network. Attention computed as dot product is called \"Luong attention\"*\n",
    "\n",
    "\n",
    "We would also like $a$ to have nice property of being probability distribution, so it would tell us with what probability each element of $x$ has a high influence on each element of $y$. This can be achieved by running $a$ through a softmax function.\n",
    "\n",
    "$p = softmax(a) \\Leftrightarrow p_{i, j} = \\frac{exp(a_{i, j})}{\\sum_k exp(a_{i, k})}$\n",
    "\n",
    "$p$ that we calculated is called attention scores and tells us what's the relationship between elements of $x$ anf $y$ - i.e. if $p_{ij}$ is close to 1, that means the relationship between $x_i$ and $y_j$ is strong.\n",
    "\n",
    "### But why is it called attention ?\n",
    "Calling it attention originates from where the concept is used. It was developed as a way to give decoder (in encoder-decoder architecture discussed earlier) a way to have access to all hidden states of encoder instead of just final one passed as context vector. If at each step of decoder we calculate attention score between it's state at this timestep and all encoder hidden states we can get information on which timesteps of encoder have the most influence on prediction that should be produced by decoder - in other words we get information to which encoder states decoder should pay the most ***attention***.\n",
    "\n",
    "\n",
    "### Queries and Values notation\n",
    "Attention inputs can also be called queries and values, since calculating attention is analogous to operation of calculating match score between set of queries (i.e. to the database) and set of values (i.e. possible search results stored in this database. Conceptually, it can be visualized as how Google works. \n",
    "Let's say that Google has database of 3 websites: $v_1 = $\"www.motocycles.com\",  $v_2 = $ \"www.cars.com\", $v_3 = $ \"www.airplanes.com\" and at given moment receives 2 queries: $q_1 = $\"What's best engine?\", $q_2 = $\"Is Ford good car?\".\n",
    "\n",
    "For $q_1$ attention scores would be relatively similar for all values since all of the motorcycles, cars and airplanes have engines, but for $q_2$, $v_2 = $ \"www.cars.com\" would definitely have the highest score, so our attention scores might look like:\n",
    "\n",
    "| $v_1$| $v_2$| $v_3$|\n",
    "| ---  | ---  | ---  |\n",
    "| 0.33 | 0.33 | 0.34 |\n",
    "| 0.01 | 0.98 | 0.01 |  \n",
    "\n",
    "\n",
    "### Attention layer\n",
    "As explained earlier, we would want to give decoder access to all previous states of encoder along with ability to pay attention only to subsection of it, that is the most important at given step. In order to do that, attention layer needs to be created. Attention layer is implementation of concept described in previous paragraph - it takes query and value, then calculates attention scores between them. Then it multiplies attention by values vector, producing a matrix where for each query element it's corresponding attention vector contains value elements scaled by attention scores. If attention layer decided to pay low attention to given element$^{\\bf*}$, it will be scaled down, otherwise it will remain close to it's original value.\n",
    "\n",
    "$^{\\bf*}$element from values vector\n",
    "\n",
    "***calculating attention_layer(query, value):***\n",
    "\n",
    "$raw\\_attention\\_scores = attention(query, value)$\n",
    "\n",
    "let's use simple attention function:\n",
    "\n",
    "$raw\\_attention\\_scores = query \\cdot value$\n",
    "\n",
    "$attention\\_scores\\_distribution = softmax(raw\\_attention\\_scores)$\n",
    "\n",
    "$attention\\_output = attention\\_scores\\_distribution \\cdot value$\n",
    "\n",
    "\n",
    "### Addinig attention to encoder-decoder\n",
    "There are multiple ways to incorporate attention layer into encoder-decoder architecture.\n",
    "- One of them is to at each timestep $t$ of decoder calculate attention values between decoder state at previous timestep and encoder hidden states  - $attention(decoder\\_hidden\\_state_{t-1}, encoder\\_hidden\\_states)$ and pass attention vlaues combined with $decoder\\_hidden\\_state_{t-1}$ to next decoder timestep\n",
    "- Another one is to leave decoder as is, but add additional layer on top of it that will take decoder output and attention values as input, producing final prediction. This approach is implemented below. \n",
    "\n",
    "It extends basic LSTM network architecture by adding attention layer with encoder outputs being values and decoder outputs being queries. Then attention vector (being encoder outputs scaled by attention) along with decoder outputs are passed through dense network in order to create a prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-09T13:18:06.092495100Z"
    },
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "oLYVR6QoFDijxHXaShupQq",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "input_timesteps = 5\n",
    "output_timesteps = 2\n",
    "batch_size = 1\n",
    "\n",
    "encoder_inputs =  tf.keras.layers.Input(shape=(input_timesteps, 1))\n",
    "encoder = tf.keras.layers.LSTM(10, return_state=True, return_sequences=False, name=\"encoder\")\n",
    "# state_h and state_c are context vector\n",
    "# encoder_outputs are discarded (they are the same as state_h since LSTM is used)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "# using hidden state h from encoder as inputs at each decoder timestep\n",
    "# decoder =  tf.keras.layers.RepeatVector(output_timesteps)(state_h)\n",
    "# using zeros as inputs at each decoder timestep\n",
    "decoder =  tf.keras.layers.RepeatVector(output_timesteps)(tf.zeros((batch_size, 1)))\n",
    "\n",
    "decoder_lstm =  tf.keras.layers.LSTM(10, return_sequences=True, return_state=False, name=\"decoder\")\n",
    "decoder_outputs = decoder_lstm(decoder, initial_state=[state_h, state_c])\n",
    "\n",
    "attention = tf.keras.layers.Attention(name=\"attention\")\n",
    "attention_outputs = attention([decoder_outputs, encoder_outputs])\n",
    "\n",
    "dense_inputs = tf.keras.layers.Concatenate()([attention_outputs, decoder_outputs])\n",
    "# using dense layer to aggregate 10-D LSTM output\n",
    "# using TimeDistributed to make it clear that dense layer is applied to every timestep separately\n",
    "# but it does not make any real difference (i believe)\n",
    "out =  tf.keras.layers.Dense(1)(dense_inputs)\n",
    "model =  tf.keras.models.Model(encoder_inputs, out)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "7A5x7boKNw9Im9kmZlJ8r0",
     "type": "MD"
    }
   },
   "source": [
    "## Transformers\n",
    "### General Idea\n",
    "Transformers are based on encoder-decoder architecture with attention, but swap out LSTMs with attention-based mechanism. This enables Transformers to overcome disadvantages of Recurrent Neural Networks, caused by their sequential nature:\n",
    "- impossibility of parallelizing calculations - processing of timestep $t_n$ can be started only after timestep $t_{n-1}$ is already processed\n",
    "-  limited possibility to capture long-term dependencies - autoencoder with attention that is based on LSTMs calculates single hidden state that is passed to decoder along with attention vector (which itself is based on hidden states), so long-term dependencies in data have significant possibility of being lost when dealing with long sequences\n",
    "\n",
    "Transfmers eliminate these downsides by calculating attention-based encoded values for each timestep (so each timestep can be processed independently of others) and then passing it to decoder using more complicated attention mechanism that better captures long-term dependencies.\n",
    "\n",
    "### Transformer architecture\n",
    "Both encoder and decoder start with 2 common building blocks: \n",
    "- Component that processes input by creating embedding and capturing temporal relationships between input elements **(1)**\n",
    "- Attention-based component that swaps out LSTMs from previously described autoencoder architecture and works by calculating attention with input vector passed as key as well as query (self attention) **(2)**\n",
    "\n",
    "Encoder also incorporates dense layer before passing data to decoder. **(3)**\n",
    "\n",
    "Decoder combines it's current output with context passed from encoder using multi-head attention mechanism **(4)** and passes it through dense layer before outputting prediction **(5)**.\n",
    "\n",
    "![Transformer architecture diagram](./images/attn_is_all.jpg)\n",
    "\n",
    "\n",
    "Both encoder and decoder might consist of multiple layers, where layer $n$ takes output of layer $n-1$ as input. Diagram below shows how transformer with 4 layers of encoder and decoder looks like.\n",
    "\n",
    "![Transformer layers \"unrolled\"](./images/unroll.png)\n",
    "\n",
    "Next paragraphs will describe core concepts necessary to fully understand Transformers, then each component will be described in detail and finally simple implementation will be provided.\n",
    "### Core concepts\n",
    "**Note:** In order for all the math to work out, all inputs and outputs should have the same dimensionality - it's called $d_{model}$. For example if input layer outputs 10-element vector, then also encoder and decoder should output 10-element vectors, so $d_{model} = 10 $\n",
    "#### Linear transformation / layer\n",
    "Linear layer is used within all attention-based mechanisms here, so just to clarify - applying linear transformations to $x$ means multiplying $x$ by some learnable weight matrix $w$, so $linear\\_layer(x) = x \\cdot w$. Or in other words - linear layer is just a dense layer, but without activation function.\n",
    "#### Postional encoding\n",
    "Layers used throughout the Transformer architecture see their input as a set of vectors, with no order, since it does not incorporate any kind of recurrence or convolution, so we need a way to gain insight into how data points are related in time domain. Positional encoding provides that ability by adding encoded position value to embeddings before passing it as input to encoder/decoder. If we were to use simplest possible way to encode position we would use $encoding(x_i)=i$. Here we are going to use a little more complicated function (that does not increase with $i$ and is periodic, so model has better chance of grasping relative dependencies between timestamps). Exact function will be stated when implementing input layer, but general concept is as follows:\n",
    "\n",
    "Input for positional encoding layer will be $n_{timesteps}$ x $d_{model}$, we want to add encoded position for each timestep, so single encoding (for $i$-th timestep) should be vector with $d_{model}$ elements. The idea is to create $d_{model}$ sine/cosine functions, each with different frequency and calculate vector where element at each position is value of $sin(i)$/$cos(i)$ with corresponding frequency. \n",
    "\n",
    "For example if we used frequencies $f = [2\\pi, 4\\pi, ... , 2 \\cdot d_{model} \\cdot \\pi] $ positional embeddings for timestep $i$ would be $pe = [sin(i, frequency=2\\pi), sin(i, frequency=4\\pi), ..., sin(i, frequency=2 \\cdot d_{model} \\cdot \\pi)]$ or we can acheive the same thing by rewriting $pe$ as $[sin(\\frac{i}{2\\pi}), sin(\\frac{i}{4\\pi}), ..., sin(\\frac{i}{2 \\cdot d_{model} \\cdot \\pi})]$\n",
    "\n",
    "Example above pretty much states how positional encoding works. Only difference from real algorithm is that alternating sin and cos should be used -  $pe = [sin(\\frac{i}{2\\pi}), cos(\\frac{i}{4\\pi}), ..., sin(\\frac{i}{2 \\cdot (d_{model} - 1) \\cdot \\pi}), cos(\\frac{i}{2 \\cdot d_{model} \\cdot \\pi})]$\n",
    "#### Little more on attention\n",
    "Transformer model introduces 2 important attention mechanisms: \n",
    "- **Masked self attention** that works by passing the same vector (after linear transformation) as Key, Query and Value to the attention layer. Self attention layer essentially calculates how related are elements of given vector (something like autocorrelation). Masked part comes from the fact, that when decoding not all attention scores should be considered in order to avoid lookahead (at timestep $n$ only scores for timesteps $< n$ should be calculated, attention scores for timesteps $> n$ should be set to 0).\n",
    "\n",
    "![Masked attention diagram](./images/obraz_2023-09-27_185212952.png)\n",
    "\n",
    "- **Mulit-head attention** that is just a fancy name for calculating multiple attention vectors using different parameters for the same Query, Key and Value, then concatenating them and passing through linear layer. Also all Queries, Keys and Values are passed through different linear layer before entering one of attention layers. Each attention layer is called attention head. Multi-head attention layer's output is calculated as\n",
    "$ MultiHead(Q,K,V) = Concatenate(head_1, ..., head_h) $, where $ head_i = Attention (Q \\cdot W_i^Q, K \\cdot W_i^K, V \\cdot W_i^V) $ and \n",
    "$W_i^Q, W_i^K, W_i^V$ are weight matrices of size $d_{model}$ x $d_{weights}$, where size of $d_{weights}$ is $\\frac{d_{model}}{h}$, because after concatenating output of all heads, output of multi-head attention layer must be of size $d_{model}$.\n",
    "\n",
    "#### Residual connections and normalization\n",
    "Since model is quite complicated, a few concepts need to be applied for it to work properly. Each sublayer of the model (Feed-forward and attention layers) needs to be \"wrapped\" into set of 2 operations:\n",
    "1. Residual connection - that for each sublayer that takes input $x$ does simple operation of adding layer's input to it's output $Residual(x, sublayer) = sublayer(x) + x$. This operation creates way for input data to still be represented in output of sublayer, so sublayer only has to take care of it's assigned task (without need to preserve \"memory\"). In diagrams residual connections are represented as \"loops\" around given layer.\n",
    "![Residual connection](./images/residual.jpg)\n",
    "\n",
    "2. Layer normalization - After adding each sublayer's output via residual connection, final output needs to be normalized. It is done by subtracting the mean and dividing by standard deviation.\n",
    " \n",
    "### Architecture of components\n",
    "#### Input layers (embedding + positional encoding)\n",
    "Input layer is the same for encoder and decoder. It consists of 2 sublayers: embedding and positional encoding.\n",
    "\n",
    "***Embedding***\n",
    "\n",
    "In case of NLP embedding layer would convert input tokens to vectors, but here (since we are woriking on continous time series data) we'll use dense layer in order to transform input data into vector of size $d_{model}$.\n",
    "\n",
    "***Positinal Encoding***\n",
    "\n",
    "As defined earlier, our goal is to create layer that adds values of certain sine/cosine functions to outputs of embedding layer in order to inject temporal information into our model. When implementing positional encoding operation, following formula will be used:\n",
    "\n",
    "$$ PE(pos, 2i) = sin(\\frac{pos}{10000^{2i / d_{model}}}) $$\n",
    "$$ PE(pos, 2i + 1) = cos(\\frac{pos}{10000^{2i / d_{model}}}) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-09T13:57:13.748550600Z",
     "start_time": "2023-10-09T13:57:13.665487600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         1.         0.         1.        ]\n",
      " [0.84147098 0.54030231 0.00999983 0.99995   ]]\n",
      "[0.0, 1.0, 0.0, 1.0]\n",
      "[0.8414709848078965, 0.5403023058681398, 0.009999833334166664, 0.9999500004166653]\n"
     ]
    }
   ],
   "source": [
    "def postional_encoding(timesteps, d_model):\n",
    "    # output of embedding layer is sized d_model x timesteps\n",
    "    # let's create matrix that will be added to it\n",
    "    positions = np.arange(timesteps)[:, np.newaxis] # [0, 1, ...] casted to shape timesteps x 1 \n",
    "    depths = np.arange(0, d_model, 2)[np.newaxis, :]/d_model  # [0/d_model, 2/d_model, ...] casted to shape 1 x d_model/2\n",
    "    angle_rates = positions / (10000 ** depths) # timesteps x d_model/2\n",
    "    \n",
    "    encoding = np.zeros((timesteps, d_model))\n",
    "    encoding[:, 0::2] = np.sin(angle_rates)\n",
    "    # if d_model is odd, cosine function will be used 1 time less than sine\n",
    "    if d_model%2 == 0 :\n",
    "        encoding[:, 1::2] = np.cos(angle_rates)\n",
    "    else:\n",
    "        encoding[:, 1::2] = np.cos(angle_rates)[:, :-1]\n",
    "        \n",
    "    \n",
    "    return encoding\n",
    "\n",
    "print(postional_encoding(2, 4))\n",
    "# at timestep = 0\n",
    "print([np.sin(0), np.cos(0), np.sin(0), np.cos(0)])\n",
    "# at timestep = 1\n",
    "print([np.sin(1/(10000**(0/4))), np.cos(1/(10000**(0/4))), np.sin(1/(10000**(2/4))), np.cos(1/(10000**(2/4)))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Class below implements common part of encoder and decoder that processes inputs. It incorporates dense layer as embedding and positional encoding defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-09T13:57:15.316348700Z",
     "start_time": "2023-10-09T13:57:14.875827300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TransformerInput(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model):\n",
    "    super().__init__()\n",
    "    self.d_model = d_model\n",
    "    self.embedding = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "  def call(self, x):\n",
    "    length = tf.shape(x)[1]\n",
    "    x = self.embedding(x)\n",
    "    x = x + postional_encoding(length, self.d_model)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Feed Forward layers\n",
    "Implementation below follows Feed Forward layer's architecture from original Transformer paper, which is Dense layer with ReLu activation, followed by linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-09T14:04:44.965694500Z",
     "start_time": "2023-10-09T14:04:44.715700100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, dff):\n",
    "        super().__init__()\n",
    "        self.seq = tf.keras.Sequential([\n",
    "          tf.keras.layers.Dense(dff, activation='relu'),\n",
    "          tf.keras.layers.Dense(d_model),\n",
    "        ])\n",
    "    def call(self, x):\n",
    "        x = self.seq(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "#### Residual connection and normalization\n",
    "Implementation below is a wrapper that takes sublayer as input and outputs normalized result of adding input with sublayer's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-09T13:57:16.251525600Z",
     "start_time": "2023-10-09T13:57:16.142153Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class AddNormLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, sublayer):\n",
    "        # this class implements residual connection around sublayer\n",
    "        # along with normalization layer\n",
    "        super().__init__()\n",
    "        self.sublayer = sublayer\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()\n",
    "\n",
    "    def call(self, x, *args):\n",
    "        sublayer_output = self.sublayer(x, *args)\n",
    "        x = self.add([x, sublayer_output])  # residual connection\n",
    "        x = self.layernorm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Encoder\n",
    "Implementation is divided into 2 classes:\n",
    "- EncoderLayer class that implements self-attention and feed forward network, both wrapped into residual connections with normalization layer\n",
    "- Encoder class that stacks EncoderLayers on top of input layer consisting of embedding and positional encoding\n",
    "![Encoder](./images/encoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-09T13:57:16.739903900Z",
     "start_time": "2023-10-09T13:57:16.598311800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff):\n",
    "    super().__init__()\n",
    "    self.mha=tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "    self.self_attention = lambda x: self.mha(query=x,value=x,key=x)\n",
    "    self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "  def call(self, x):\n",
    "    x = AddNormLayer(self.self_attention)(x)\n",
    "    x = AddNormLayer(self.ffn)(x)\n",
    "    return x\n",
    "  \n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, n_layers, d_model, num_heads, dff):\n",
    "    super().__init__()\n",
    "    self.n_layers = n_layers\n",
    "    self.input_layer = TransformerInput(d_model)\n",
    "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff) for _ in range(n_layers)]\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.input_layer(x)\n",
    "    for i in range(self.n_layers):\n",
    "        x = self.enc_layers[i](x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Decoder\n",
    "Implementation is divided into 2 classes:\n",
    "- DecoderLayer class that implements masked self-attention, cross attention with encoder output as context and feed forward network, all wrapped into residual connections with normalization layer\n",
    "- Decoder class that stacks DecoderLayer on top of input layer consisting of embedding and positional encoding\n",
    "![Decoder](./images/decoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-09T13:57:17.521160500Z",
     "start_time": "2023-10-09T13:57:16.974287400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff):\n",
    "    super().__init__()\n",
    "    self.mha=tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "    self.self_attention = lambda x: self.mha(query=x,value=x,key=x, use_causal_mask = True)\n",
    "    self.cross_attention = lambda x, context: self.mha(query=x,value=context,key=context)\n",
    "    self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "  def call(self, x, context):\n",
    "    x = AddNormLayer(self.self_attention)(x)\n",
    "    x = AddNormLayer(self.cross_attention)(x, context)\n",
    "    x = AddNormLayer(self.ffn)(x)\n",
    "    return x\n",
    "  \n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, n_layers, d_model, num_heads, dff):\n",
    "    super().__init__()\n",
    "    self.n_layers = n_layers\n",
    "    self.input_layer = TransformerInput(d_model)\n",
    "    self.dec_layers = [DecoderLayer(d_model, num_heads, dff) for _ in range(n_layers)]\n",
    "\n",
    "  def call(self, x, context):\n",
    "    x = self.input_layer(x)\n",
    "    for i in range(self.n_layers):\n",
    "        x = self.dec_layers[i](x, context)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Complete Transfomer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-09T13:57:17.818033Z",
     "start_time": "2023-10-09T13:57:17.489912400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, n_layers, d_model, num_heads, dff, output_size):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(n_layers, d_model, num_heads, dff)\n",
    "        self.decoder = Decoder(n_layers, d_model, num_heads, dff)\n",
    "        self.final_layer = tf.keras.layers.Dense(output_size)\n",
    "\n",
    "    def call(self, data):\n",
    "        encoder_inputs, decoder_inputs = data # decoder inputs are outputs shifted right\n",
    "        context = self.encoder(encoder_inputs)\n",
    "        x = self.decoder(decoder_inputs, context)\n",
    "        x = self.final_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-09T14:06:56.858052500Z",
     "start_time": "2023-10-09T14:06:55.623284100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder_5 (Encoder)         multiple                  125180    \n",
      "                                                                 \n",
      " decoder_4 (Decoder)         multiple                  125168    \n",
      "                                                                 \n",
      " dense_123 (Dense)           multiple                  13        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 250361 (977.97 KB)\n",
      "Trainable params: 250361 (977.97 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n_layers = 4\n",
    "d_model = 12\n",
    "dff = 51\n",
    "num_heads = 2\n",
    "output_size = 1\n",
    "input_size = 2\n",
    "\n",
    "input_timesteps = 5\n",
    "output_timesteps = 2\n",
    "batch_size = 1\n",
    "inputs = np.zeros((batch_size, input_timesteps, input_size))\n",
    "outputs = np.zeros((batch_size, output_timesteps, output_size))\n",
    "\n",
    "transformer = Transformer(n_layers, d_model, dff, num_heads, output_size)\n",
    "x = transformer((inputs, outputs))\n",
    "transformer.summary()"
   ]
  }
 ],
 "metadata": {
  "datalore": {
   "base_environment": "default",
   "computation_mode": "JUPYTER",
   "package_manager": "pip",
   "packages": [],
   "report_row_ids": [],
   "version": 3
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
